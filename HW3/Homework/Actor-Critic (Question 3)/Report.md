# 结果比较

### **REINFORCE-1（N=1）**

- **训练特点**：每个策略更新使用 **1条轨迹**
- **回报表现**：
  - 初期：波动极大，回报值在很大范围内震荡
  - 中期：收敛缓慢，可能出现长时间的平台期
  - 最终：可能收敛到次优解，平均回报较低

### **REINFORCE-10（N=10）**

- **训练特点**：每个策略更新使用 **10条轨迹**
- **回报表现**：
  - 初期：相对稳定，回报平滑上升
  - 中期：收敛速度较快，学习曲线更平滑
  - 最终：更容易达到更高且稳定的平均回报

# 原因解释

**REINFORCE-1** 的梯度估计方差极高，因为：

- 单条轨迹的累计回报 `Σr` 随机性大
- 优势函数估计不准确导致更新方向噪声大

**REINFORCE-10** 通过平均多条轨迹的梯度估计，显著降低了方差。

# Q学习网络

效果评价：在有限的100k step中三者很接近，可能double q更好

**Singlecritic（单 Critic ）**：若采用单 Critic 网络估计 Q 值，理论上因网络更新与策略优化的耦合，可能出现高估。但从曲线看，其收敛后 `q_values` 未过度偏离合理值，可能因算法（如 SAC 本身的正则化、目标网络延迟更新等机制 ）一定程度抑制了偏差。若它实际高估偏差更小，或因网络结构简单、更新更 “克制”，反而让 `eval_return` 更稳定，最终表现更好。

**Doubleq（双 Critic ）**：双 Critic 本就是为缓解高估偏差设计（取两个 Critic 估计的最小值更新策略 ）。从 `q_values` 看，曲线波动与单 Critic 接近，说明双 Critic 有效抑制了高估。但 `eval_return` 没显著超越单 Critic ，可能因双 Critic 增加了计算开销（或训练初期稳定性稍弱 ），抵消部分偏差优势，导致整体收益未拉开差距。

**Clipq（截断 Q ）**：通过截断 Q 值更新幅度限制高估，`q_values` 曲线同样有波动但收敛趋势正常。若截断策略不够精准，可能导致 Q 值估计不足或过度修正，从结果看它在 `eval_return` 上稍逊于单 Critic ，或许因截断影响了价值估计的准确性，间接拉低最终回报。

# 衡量样本效率

同策略和异策略可通过达到相同性能（eval_return）时所需的样本数量衡量样本效率。